<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Operating System on UMANG JAIN</title>
        <link>https://umangja.github.io/posts/operatingsystem/</link>
        <description>Recent content in Operating System on UMANG JAIN</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Sun, 31 Jan 2021 20:51:24 +0530</lastBuildDate>
        <atom:link href="https://umangja.github.io/posts/operatingsystem/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Threads</title>
            <link>https://umangja.github.io/posts/2021/02/threads/</link>
            <pubDate>Tue, 02 Feb 2021 16:31:58 +0530</pubDate>
            
            <guid>https://umangja.github.io/posts/2021/02/threads/</guid>
            <description>WHERE DOES DIFFERENT KIND OF VARIABLES GOES
Memory of a process is divided into different memory segments :- stack, heap, data, code.
code section contain executable instructions.
Global/Stack Variable :- data
Const Variable :- code
Local Variable/Temporay Variables :- stack
Dynamically allocated memory :- heap
Pointers :- stack/heap/data. ( const pointer goes to data, non constant pointer goes to stack, ptr itself is in either stack or data but pointed memory may also be a heap)</description>
            <content type="html"><![CDATA[<p><strong>WHERE DOES DIFFERENT KIND OF VARIABLES GOES</strong></p>
<p>Memory of a process is divided into different memory segments :- stack, heap, data, code.</p>
<p>code section contain executable instructions.</p>
<p>Global/Stack Variable :- data</p>
<p>Const Variable :- code</p>
<p>Local Variable/Temporay Variables  :- stack</p>
<p>Dynamically allocated memory :- heap</p>
<p>Pointers :- stack/heap/data. ( const pointer goes to data, non constant pointer goes to stack, ptr itself is in either stack or data but pointed memory may also be a heap)</p>
<p>Threads of a process have their own stack, PC, Registers, But it shares code, data, openfiles, address space, etc.</p>
<p>When we say threads share the same address space we mean that when accessing a variable <code>foo</code> is global scope, all threads will see this <code>foo</code> variable. And similarly all threads will be able to call a exact global function.</p>
<p>Modern operating system also has a thread local storage for variable of global scope which are not shared. eg <strong>errno</strong>.</p>
<p><strong>Threads have independent call stack however memory in other thread&rsquo;s stack is still accessible.</strong></p>
<h6> Why does threads share heap. </h6>
<p>May be so that they could share data between them that is not global. But now it do need synchronization.</p>
<p>Memory allocation and deallocation have a significant overhead.</p>
<h2 id="benefits-of-multi-threading">Benefits of Multi-Threading</h2>
<ol>
<li>Efficiency</li>
<li>Responsive :- In UI</li>
<li>Resource Sharing</li>
<li>Multi-Processing</li>
</ol>
<h2 id="concurrency-vs-multi-processing">Concurrency vs Multi-processing</h2>
<p>One process can work at most one process at a time.</p>
<p>When many process are interleaved with each other. Means one processor is letting many process to make progress giving a illusion of multi-processing.</p>
<p>Multi-processing is when two process are actually running parally.</p>
<h2 id="problems-faced-by-programmers-with-multithreading">Problems faced by programmers with multithreading.</h2>
<ol>
<li>Identifying</li>
<li>Balancing</li>
<li>Data dependency</li>
<li>Testing and debugging</li>
</ol>
<h2 id="data-parallelism-vs-task-parallelism">Data Parallelism vs Task Parallelism</h2>
<p>In data Parallelism, we perform the same task on different data in parallel.</p>
<p>In Task Parallelism, we perform different task on same data in parallel.</p>
<h2 id="multi-threading-models">Multi-Threading Models</h2>
<p>So, Application Programmers work on user thread but process are actually executed on kernel thread, So we need a mapping from user thread to kernel threads.</p>
<h3 id="many-to-one-mapping">Many to One Mapping</h3>
<p>In this many user threads are mapped to only one kernel threads. It has many disadvantages.</p>
<ol>
<li>
<p>If one user thread makes a blocking call, entire process get blocked.</p>
</li>
<li>
<p>As All user thread of a program is mapped to only one kernel thread, Hence multi-processing is not possible even in multicore processors.</p>
</li>
</ol>
<p>This Allows developer to create as many threads as needed but <strong>true</strong> concurrency doesn&rsquo;t happen because only thread can be scheduled at a time.</p>
<h3 id="one-to-one-mapping">One To One Mapping</h3>
<p>In this one user thread get mapped to only one kerner thread. It solves problem of many to one mapping.</p>
<ol>
<li>
<p>If one user thread makes a blocking call, other thread of same process can still work.</p>
</li>
<li>
<p>It offers multiprocessing on multicore processors and also a better concurrency that many to one model.</p>
</li>
</ol>
<p><strong>Disadvanges</strong></p>
<ol>
<li>As while creating a kernel thread, we need to switch from user mode to kernel mode and make few system call. So creating a kernel thread has a overhead which can burden the system and may lead to depreciated performance. That&rsquo;s why most of its implementation has a restriction on the number of threads available to the user.</li>
</ol>
<h3 id="many-to-many-model">Many to Many Model</h3>
<p>In this many user thread get mapped to only many kerner thread (usually less or equal than user thread).</p>
<p>The number of kernel threads can be restricted for each application or for machine.</p>
<p>This allows developers to create as many thread as needed  and also allow better concurrency and multithreading on multi core processors.</p>
<p>Also blocking call does not lead to blocking while application.</p>
<p><strong>In pratice most of the modern system uses a two level model which is a combination of 1-1 model and many - many model</strong></p>
<h2 id="asynchronous-vs-synchronous-threading">Asynchronous vs synchronous threading</h2>
<p>In Asynchronoud threading, once the parent creates the child, the parent continues So that parent and child executes concurrently. Each thread run <strong>independently</strong> of every other thread and parent thread need not know of its child termination. Hence little data sharing between threads is needed.</p>
<p>In synchronoud threading, Parent must wait for all its children to terminate before it resumes its execution the so called <strong>fork-join</strong> strategy. Synchronoud threading usually involves great sharing of data.</p>
<h2 id="thread-libraries">Thread Libraries</h2>
<p>Thread Libraries provides the API for creating and implementing threading. Thread library can be implemented either in user space and in kernel space.</p>
<p>In user space, All code and functions exist is user space means invoking a function in the library results in calling a local function not a system call.</p>
<p>In kernel space, code and functions exist is kernel space means invoking a function in the library results in a system call.</p>
<h3 id="pthreads">Pthreads</h3>
<p>This is a <strong>specification</strong> for thread library not <strong>implementation</strong>.</p>
<p><strong>It&rsquo;s certainly better to create a thread as compared to create a new process</strong> but creating a new thread do take time and new thread will be discarded once its work is completed. <strong>Also we can allow unrestricted creating of threading as it can lead to CPU resource exhaustion.</strong></p>
<h2 id="implicit-threading">Implicit Threading</h2>
<p>It&rsquo;s become difficult for Application Programmers to make Application containing 100s to 1000s of threads. One way to solve this is to take the burden of creating threads from Programmer and let kernel do implicit threading.</p>
<p>One way to do so is <strong>Thread Pool</strong></p>
<p>Idea is to at the start of the process create a number of thread and place them in a pool where they sit and wait for work.
So if we need one thread, it awakes a thread from pool and assign work to it and once thread completes it&rsquo;s work thread returns to the pool.</p>
<ol>
<li>
<p>Awakening a thread is better and creating a thread.</p>
</li>
<li>
<p>A Thread pool limits the number of thread at a time.</p>
</li>
</ol>
<p>The number of threads in the pool can be set heuristically based on factors
such as the number of CPU s in the system, the amount of physical memory,
and the expected number of concurrent client requests</p>
<h2 id="threading-issues">Threading issues</h2>
<h3 id="the-fork-vs-exec-system-call">The fork() vs exec() system call</h3>
<p>So when we call <strong>fork()</strong>, we have to decide whether we want duplicate all threads, or we want new process to be single threaded.</p>
<p>We don&rsquo;t have any problem in callling <strong>exec()</strong> as exec replaces process space with new process so only  calling thread need to be duplicated.</p>
<p>When exec() is called after fork() then only calling thread is need to be duplicated otherwise all thread need to be duplicated.</p>
<h3 id="signal-handling">Signal Handling</h3>
<p>Signal are used to notify a process/thread of the happening of some event.</p>
<ol>
<li>
<p>Signal is generated by the occurence of some event.</p>
</li>
<li>
<p>Signal is sent to the process.</p>
</li>
<li>
<p>Process handles signal by invoking signal handler.</p>
</li>
</ol>
<p>Signal can be of two types</p>
<ol>
<li>
<p>Asynchronous Signals :- These signals are generated by external events. Ex. specific keystroke, timer expired.</p>
</li>
<li>
<p>Synchronous Signals :- These signals are generated by internal eventes Ex. Division by 0, segmentation error.</p>
</li>
</ol>
<p>Whenever a signal is generated, we have few options to deliver to</p>
<ol>
<li>
<p>Deliver it to every thread</p>
</li>
<li>
<p>Deliver it to specific thread</p>
</li>
<li>
<p>Deliver it to certain theads.</p>
</li>
<li>
<p>Have a specific thread to receive all signals and deliver it to that thread.</p>
</li>
</ol>
<p><strong>Synchronous signals need to be delivered to the thread generating the signal only</strong></p>
<p>However <strong>Asynchronoud Signal</strong> have many options.</p>
<p>Terminating signal need to be sent to all threads.</p>
<p><strong>kill system call in linux actually generates a Asynchronous signal</strong></p>
<h3 id="thread-cancelation">Thread Cancelation</h3>
<p>Thread cancelation means terminating a thread <strong>before</strong> it has completed it&rsquo;s work.</p>
<p>Thread cancelation can be two types :-</p>
<ol>
<li>
<p>Asynchronous cancelation:- One Thread imidiately terminates the target thread.</p>
</li>
<li>
<p>Deferred cancelation:- The target periodically checks <strong>at safe plces</strong> whether it should terminate or not, This allows the thread for a safe termination.</p>
</li>
</ol>
<p>Asynchronous cancelation has few problems.</p>
<ol>
<li>
<p>what if thread get terminated when it sharing data with other.</p>
</li>
<li>
<p>OS reclaims resources from cancelled threads but will not reclaim all resources. So cancelling a thread asynchronously may not free all resources.</p>
</li>
</ol>
<h3 id="thread-local-storage">Thread Local Storage</h3>
<p>Threads may need its own copy of certain data. We can store such data in TLS.</p>
<p>TLS is different from local variable as variable in TLS persist over many function calls.</p>
<p>TLS is much like static variable with only difference being TLS is local to a thread.</p>
<h3 id="scheduler-activation">Scheduler Activation</h3>
<p>A final issue to be considered with multithreaded programs concerns communication between the kernel and the thread library to help ensure the best performance.</p>
<p>Many systems implementing either the many-to-many or the two-level
model place an intermediate data structure between the user and kernel
threads. This data structure —typically known as a lightweight process, or
LWP.</p>
<p>To the user-thread library, the LWP appears to be a virtual processor on which the application can schedule a user thread to run. Each LWP is attached to a kernel thread, aand it is kernel threads that OS schedules to run on physical processors.</p>
<p>If a kernel thread blocks (such as while waiting for an I/O operation to complete), the LWP blocks as well. Up the chain, the user-level thread attached to the LWP also blocks.</p>
<p>An application may require any number of LWP s to run efficiently. Consider
a CPU -bound application running on a single processor. In this scenario, only
one thread can run at at a time, so one LWP is sufficient. An application that is
I/O -intensive may require multiple LWP s to execute, however. Typically, an LWP
is required for each concurrent blocking system call. Suppose, for example, that
five different file-read requests occur simultaneously. Five LWPs are needed,
because all could be waiting for I/O completion in the kernel. If a process has
only four LWPs, then the fifth request must wait for one of the LWP s to return
from the kernel.</p>
<p>One scheme for communication between the user-thread library and the
kernel is known as scheduler activation. It works as follows: The kernel
provides an application with a set of virtual processors ( LWP s), and the
application can schedule user threads onto an available virtual processor.
Furthermore, the kernel must inform an application about certain events. This
procedure is known as an upcall. Upcalls are handled by the thread library
with an upcall handler, and upcall handlers must run on a virtual processor.
One event that triggers an upcall occurs when an application thread is about to
block. In this scenario, the kernel makes an upcall to the application informing
it that a thread is about to block and identifying the specific thread. The kernel
then allocates a new virtual processor to the application. The application runs
an upcall handler on this new virtual processor, which saves the state of the
blocking thread and relinquishes the virtual processor on which the blocking
thread is running. The upcall handler then schedules another thread that is
eligible to run on the new virtual processor. When the event that the blocking
thread was waiting for occurs, the kernel makes another upcall to the thread
library informing it that the previously blocked thread is now eligible to run.
The upcall handler for this event also requires a virtual processor, and the kernel
may allocate a new virtual processor or preempt one of the user threads and
run the upcall handler on its virtual processor. After marking the unblocked
thread as eligible to run, the application schedules an eligible thread to run on
an available virtual processor.</p>
]]></content>
        </item>
        
        <item>
            <title>Process</title>
            <link>https://umangja.github.io/posts/2021/02/process/</link>
            <pubDate>Mon, 01 Feb 2021 14:26:32 +0530</pubDate>
            
            <guid>https://umangja.github.io/posts/2021/02/process/</guid>
            <description>TO-DO EXAMPLES OF IPC SYSTEMS communication in client - server systems
Solution 3.1  &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; As fork basically creates a duplicate of current process, identical in almost very way except a few. As it as a different pid, and parent pid. But idea is to create as identical copy as possible. Hence copy of variables, stack and heap, are created. Hence child process variable and parent process variable are now different.</description>
            <content type="html"><![CDATA[<h2 id="to-do">TO-DO</h2>
<p><strong>EXAMPLES OF IPC SYSTEMS</strong>
<strong>communication in client - server systems</strong></p>
<h2 id="solution">Solution</h2>
<h3 id="31">3.1</h3>
<p> &nbsp; &nbsp; &nbsp; &nbsp;
As fork basically creates a duplicate of current process, identical in almost very way except a few. As it as a different pid, and parent pid. But idea is to create as identical copy as possible. Hence copy of variables, stack and heap, are created. 
Hence child process variable and parent process variable are now different. 
So ans is 5.
</p>
<h3 id="32">3.2</h3>
<p>       
8</p>
<h3 id="33">3.3</h3>
<p>       
There is a lot of complications that occurs :-</p>
<ol>
<li>
<p>How to effectively share resources among different process.</p>
</li>
<li>
<p>How to effectively allow access to shared memory without different process accessing same location simultaneously.</p>
</li>
<li>
<p>Prevention of deadlocks.</p>
</li>
</ol>
<h3 id="34">3.4</h3>
<p>       
If the new context in already in register set  space, then CPU current context pointer changes to register corresponding to new context.</p>
<p>But if new context is in memory space and all register in register set are full then,
one of the register in register set is chossen and its data is saved in memory and new context is loaded in this choosen register.</p>
<h3 id="35">3.5</h3>
<p>       
As mentioned in problem 1, child process create its own copy of stack and heap. But shared memory is shared between child and parent process.</p>
<p>Because the way shared memory is implemented is that shared memory space is mapped to current process address space ( using a pointer to starting of shared memory state ) and when current process forks, child process tries to duplicate parent process address space, getting the same mapping. (pointer)</p>
<h3 id="36">3.6</h3>
<p>       
To do</p>
<h3 id="37">3.7</h3>
<p>       
To do left</p>
<h3 id="38">3.8</h3>
<p>       </p>
<ol>
<li>
<p><strong>Short-term scheduling</strong> :- This scheduling select which process from the ready queue should be selected now in order of have most efficient system. This is called frequently and thus should be very fast.</p>
</li>
<li>
<p><strong>Long-term scheduling</strong> :- This scheduling algo. selects process waiting to be executed and load them into memory(ready queue). This controls the degree of multi-programming (number of process in ready queue).</p>
</li>
<li>
<p><strong>Medium-term scheduling</strong> :- Many times it is better to remove a process from memory and hold it&rsquo;s execution and later put it back in the memory.</p>
</li>
</ol>
]]></content>
        </item>
        
    </channel>
</rss>
