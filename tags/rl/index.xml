<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RL on UMANG JAIN</title>
    <link>https://umangja.github.io/tags/rl/</link>
    <description>Recent content in RL on UMANG JAIN</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Fri, 19 Feb 2021 11:43:11 +0530</lastBuildDate><atom:link href="https://umangja.github.io/tags/rl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>UCRL2</title>
      <link>https://umangja.github.io/posts/2021/02/ucrl2/</link>
      <pubDate>Fri, 19 Feb 2021 11:43:11 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/02/ucrl2/</guid>
      <description>Introduction In a Markov Decision Process (MDP) M with finite space S and finite action space A, a learner in some state &amp;rsquo;s&#39; choose some action &amp;lsquo;a&amp;rsquo; and receives a reward &amp;lsquo;r&amp;rsquo; independently from some probability distribution in [0-1] with mean r_hat(s,a). And then according to some transition probability it transits to some state s&#39;.
We need to form such a learner that always chooses optimal action which maximizes the expected reward.</description>
    </item>
    
    <item>
      <title>Lecture 6</title>
      <link>https://umangja.github.io/posts/2021/01/lecture-6/</link>
      <pubDate>Wed, 27 Jan 2021 12:28:25 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/01/lecture-6/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Model Free Control</title>
      <link>https://umangja.github.io/posts/2021/01/model-free-control/</link>
      <pubDate>Sat, 23 Jan 2021 11:45:00 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/01/model-free-control/</guid>
      <description>Slides
Lecture
Introduction In the last lecture we did model free prediction now we want to do model free control means we want to find the optimal policy.
If we go back to Lecture 3 :- Introduction to dynamic programming. In there the path we followed is that first we learnt how to do policy evaluation and find state value function and then take a greedy step to form new policy acc.</description>
    </item>
    
    <item>
      <title>Model Free Prediction</title>
      <link>https://umangja.github.io/posts/2021/01/model-free-prediction/</link>
      <pubDate>Fri, 22 Jan 2021 22:31:33 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/01/model-free-prediction/</guid>
      <description>Slides
Lecture
Introduction In previous Lectures we dealt with MDPs means environment is fully observable means we know the model.
Means we know about transition probabilities and rewards.
But now we don&amp;rsquo;t know the model and we don&amp;rsquo;t know about transition probabilities and rewards.
In this lecture we are doing prediction means given a polcy we need to find value function for that policy.
Monte Carlo Learning (MC) Idea is to find learn from complete episodes and find mean g_t for any state will be equal to it&amp;rsquo;s value function.</description>
    </item>
    
    <item>
      <title>Introdution to Dynamic Programming</title>
      <link>https://umangja.github.io/posts/2021/01/introdution-to-dynamic-programming/</link>
      <pubDate>Fri, 22 Jan 2021 17:01:37 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/01/introdution-to-dynamic-programming/</guid>
      <description>Slides
Lecture
Introduction In this lecture, We are working with Planing means someone gives us how the environment works i.e. someone tells us rewards function and State probability transition
And we have MDPs means environment is fully observable.
Policy Evaluation Now, we are given a policy $ \pi $ and our MDP = &amp;lt; S,A,P,R, $ \gamma $ &amp;gt;
and we need to find the value function for that policy</description>
    </item>
    
    <item>
      <title>Markov Decision Process</title>
      <link>https://umangja.github.io/posts/2021/01/markov-decision-process/</link>
      <pubDate>Fri, 22 Jan 2021 00:46:24 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/01/markov-decision-process/</guid>
      <description>Slides
Lecture
What is Markov Decision Process(MDP) MDP is a process where the environment is fully observable. i.e. current state fully characterizes the future process/outcomes
POMDP can be converted to MDP
$$ P[S_{t+1}|S_t] = P[S_{t+1}|S_t,S_{t-1},S_{t-2}..] $$
What is a Markov Process (MP) Markov process/Markov chain is a sequence of states with markov properties
MP = &amp;lt; S,P &amp;gt;
S = set of states
P_{ss^{&#39;}} = probability of transition from state s to s^{&#39;}.</description>
    </item>
    
    <item>
      <title>Introduction to Reinforcement Learning</title>
      <link>https://umangja.github.io/posts/2021/01/introduction-to-reinforcement-learning/</link>
      <pubDate>Thu, 21 Jan 2021 18:52:57 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/01/introduction-to-reinforcement-learning/</guid>
      <description>LINKS Slides Video Lecture Link Short Explanations Rewards It is the central quantity that tells us how good are we doing at time t. Our job is to maximize this reward at the end of the episode.
It is beleived that all goals that be achieved by maximizing cumulative total reward.
History $ H_t = O_1R_1A_1O_2R_2A_2&amp;hellip;.O_tR_tA_t $
State state is a summary of history.
$ S_t = F(H_t) $
Agent State Agent state is the information agent uses to predict next action.</description>
    </item>
    
  </channel>
</rss>
