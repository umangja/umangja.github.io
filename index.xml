<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>UMANG JAIN</title>
    <link>https://umangja.github.io/</link>
    <description>Recent content on UMANG JAIN</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Fri, 19 Feb 2021 11:43:11 +0530</lastBuildDate><atom:link href="https://umangja.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>UCRL2</title>
      <link>https://umangja.github.io/posts/2021/02/ucrl2/</link>
      <pubDate>Fri, 19 Feb 2021 11:43:11 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/02/ucrl2/</guid>
      <description>Introduction In a Markov Decision Process (MDP) M with finite space S and finite action space A, a learner in some state &amp;rsquo;s&#39; choose some action &amp;lsquo;a&amp;rsquo; and receives a reward &amp;lsquo;r&amp;rsquo; independently from some probability distribution in [0-1] with mean r_hat(s,a). And then according to some transition probability it transits to some state s&#39;.
We need to form such a learner that always chooses optimal action which maximizes the expected reward.</description>
    </item>
    
    <item>
      <title>Operator Overloading</title>
      <link>https://umangja.github.io/posts/2021/02/operator-overloading/</link>
      <pubDate>Tue, 02 Feb 2021 20:54:08 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/02/operator-overloading/</guid>
      <description>Operator overloading is a way to make an operator work in different ways based on types of operants.
Some Rules about Operator overloading :-
  New operators can not be created.
  Precedence and associativity of the operators cannot be changed.
  Overloaded operators cannot have default arguments except the function call operator ().
  Operators cannot be overloaded for built in types only. At least one operand must be used defined type.</description>
    </item>
    
    <item>
      <title>Threads</title>
      <link>https://umangja.github.io/posts/2021/02/threads/</link>
      <pubDate>Tue, 02 Feb 2021 16:31:58 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/02/threads/</guid>
      <description>WHERE DOES DIFFERENT KIND OF VARIABLES GOES
Memory of a process is divided into different memory segments :- stack, heap, data, code.
code section contain executable instructions.
Global/Stack Variable :- data
Const Variable :- code
Local Variable/Temporay Variables :- stack
Dynamically allocated memory :- heap
Pointers :- stack/heap/data. ( const pointer goes to data, non constant pointer goes to stack, ptr itself is in either stack or data but pointed memory may also be a heap)</description>
    </item>
    
    <item>
      <title>Function Overloading</title>
      <link>https://umangja.github.io/posts/2021/02/function-overloading/</link>
      <pubDate>Mon, 01 Feb 2021 19:00:44 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/02/function-overloading/</guid>
      <description>Function overloading is a way to exploit polymorphism in C++.
Function Overloading is the feature where two functions with same name but with different parameter type, no. of parameter can behave as two seperate functions.
Function overloading is compile time polymorphism
 Possible with type of paramter and number of parameter. But  int fun(int a); int fun(double a); fun(2.0) // error as float or double can be implicitely converted to int.</description>
    </item>
    
    <item>
      <title>Process</title>
      <link>https://umangja.github.io/posts/2021/02/process/</link>
      <pubDate>Mon, 01 Feb 2021 14:26:32 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/02/process/</guid>
      <description>TO-DO EXAMPLES OF IPC SYSTEMS communication in client - server systems
Solution 3.1  &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; As fork basically creates a duplicate of current process, identical in almost very way except a few. As it as a different pid, and parent pid. But idea is to create as identical copy as possible. Hence copy of variables, stack and heap, are created. Hence child process variable and parent process variable are now different.</description>
    </item>
    
    <item>
      <title>Lecture 6</title>
      <link>https://umangja.github.io/posts/2021/01/lecture-6/</link>
      <pubDate>Wed, 27 Jan 2021 12:28:25 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/01/lecture-6/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Atcoder ABC 189</title>
      <link>https://umangja.github.io/posts/2021/01/atcoder-abc-189/</link>
      <pubDate>Sat, 23 Jan 2021 21:30:49 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/01/atcoder-abc-189/</guid>
      <description>Problem - A Problem Link
Simply check if S[0]==S[1]==S[2]
Problem - B Problem Link
Simply keep calculating alcohol volume in your body and see when does it becomes strickly greater than x.
To avoid precision error use
X &amp;gt; (Y/100) &amp;lt;==&amp;gt; X*100 &amp;gt; Y  code Atcoder solution Link
Problem - C Problem Link
In every subsegment it&amp;rsquo;s optimal to choose maximum value of x as x has to be less than equal to every value in segment.</description>
    </item>
    
    <item>
      <title>Model Free Control</title>
      <link>https://umangja.github.io/posts/2021/01/model-free-control/</link>
      <pubDate>Sat, 23 Jan 2021 11:45:00 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/01/model-free-control/</guid>
      <description>Slides
Lecture
Introduction In the last lecture we did model free prediction now we want to do model free control means we want to find the optimal policy.
If we go back to Lecture 3 :- Introduction to dynamic programming. In there the path we followed is that first we learnt how to do policy evaluation and find state value function and then take a greedy step to form new policy acc.</description>
    </item>
    
    <item>
      <title>Model Free Prediction</title>
      <link>https://umangja.github.io/posts/2021/01/model-free-prediction/</link>
      <pubDate>Fri, 22 Jan 2021 22:31:33 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/01/model-free-prediction/</guid>
      <description>Slides
Lecture
Introduction In previous Lectures we dealt with MDPs means environment is fully observable means we know the model.
Means we know about transition probabilities and rewards.
But now we don&amp;rsquo;t know the model and we don&amp;rsquo;t know about transition probabilities and rewards.
In this lecture we are doing prediction means given a polcy we need to find value function for that policy.
Monte Carlo Learning (MC) Idea is to find learn from complete episodes and find mean g_t for any state will be equal to it&amp;rsquo;s value function.</description>
    </item>
    
    <item>
      <title>Introdution to Dynamic Programming</title>
      <link>https://umangja.github.io/posts/2021/01/introdution-to-dynamic-programming/</link>
      <pubDate>Fri, 22 Jan 2021 17:01:37 +0530</pubDate>
      
      <guid>https://umangja.github.io/posts/2021/01/introdution-to-dynamic-programming/</guid>
      <description>Slides
Lecture
Introduction In this lecture, We are working with Planing means someone gives us how the environment works i.e. someone tells us rewards function and State probability transition
And we have MDPs means environment is fully observable.
Policy Evaluation Now, we are given a policy $ \pi $ and our MDP = &amp;lt; S,A,P,R, $ \gamma $ &amp;gt;
and we need to find the value function for that policy</description>
    </item>
    
  </channel>
</rss>
